# project_scraper.py (v7.1 - Robust Traversal Edition)
import os
import argparse
import fnmatch
from collections import defaultdict
import time
import yaml  # éœ€è¦ pip install pyyaml
from pathlib import Path

# ==============================================================================
# æ ¸å¿ƒé€»è¾‘ (V7.1)
# ==============================================================================
class ProjectScraper:
    def __init__(self, project_path, output_file, config_path):
        self.project_path = Path(project_path).resolve()
        self.output_file = output_file
        self.config = self._load_config(config_path)
        self.files_by_category = defaultdict(list)
        self.total_files_scanned = 0
        self.max_file_size = self.config.get('max_file_size_kb', 2048) * 1024

    def _log(self, message, indent=0):
        print(f"{'  ' * indent}{message}")

    def _load_config(self, config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            self._log(f"[ERROR] é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}", 1)
            exit(1)
        except yaml.YAMLError as e:
            self._log(f"[ERROR] è§£æYAMLé…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}", 1)
            exit(1)

    def _is_binary(self, file_path):
        try:
            with open(file_path, 'rb') as f:
                return b'\0' in f.read(1024)
        except IOError:
            return True

    def _is_ignored(self, path_obj: Path):
        relative_path_str = str(path_obj.relative_to(self.project_path)).replace(os.sep, '/')
        for pattern in self.config.get('ignore_patterns', []):
            if fnmatch.fnmatch(relative_path_str, pattern):
                return True
        return False

    def _classify_path(self, path_obj: Path):
        relative_path_str = str(path_obj.relative_to(self.project_path)).replace(os.sep, '/')
        patterns_map = self.config.get('core_file_patterns', {})
        for category, patterns in patterns_map.items():
            for pattern in patterns:
                if fnmatch.fnmatch(relative_path_str, pattern):
                    return category, pattern
        return None, None

    def scrape(self):
        start_time = time.time()
        self._log(f"ğŸš€ [Architect's Insight v7.1] å¼€å§‹æ‰«æé¡¹ç›®: {self.project_path}")
        self._log(f"[*] ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}", 1)

        # --- KEY CHANGE: Reverted to os.walk for robust directory pruning ---
        for root, dirs, files in os.walk(self.project_path, topdown=True):
            root_path = Path(root)

            # Prune ignored directories IN-PLACE to prevent os.walk from entering them
            original_dirs = list(dirs)
            dirs[:] = [d for d in original_dirs if not self._is_ignored(root_path / d)]

            for file in files:
                self.total_files_scanned += 1
                file_path_obj = root_path / file

                # Final check for files that might be ignored by pattern
                if self._is_ignored(file_path_obj):
                    continue

                category, pattern = self._classify_path(file_path_obj)
                
                if category:
                    try:
                        # Safety checks before processing
                        if file_path_obj.stat().st_size > self.max_file_size:
                            self._log(f"[!] Skipped (too large): {file_path_obj.relative_to(self.project_path)}", 3)
                            continue
                        if self._is_binary(file_path_obj):
                            self._log(f"[!] Skipped (binary): {file_path_obj.relative_to(self.project_path)}", 3)
                            continue

                        self.files_by_category[category].append(file_path_obj)
                    except OSError as e:
                        # Catch any residual access errors on specific files
                        self._log(f"[!] Error stating file {file_path_obj.relative_to(self.project_path)}: {e}", 3)


        self._write_output()
        self._print_summary(time.time() - start_time)

    def _write_output(self):
        self._log(f"[*] æ­£åœ¨èšåˆå…³é”®æ¶æ„æ–‡ä»¶...", indent=1)
        with open(self.output_file, 'w', encoding='utf-8', errors='ignore') as out_f:
            category_order = self.config.get('category_order', [])
            
            for category in category_order:
                if category not in self.files_by_category:
                    continue

                self._log(f"Processing category: {category}", indent=2)
                
                sorted_files = sorted(self.files_by_category[category])
                
                for file_path in sorted_files:
                    relative_path_str = str(file_path.relative_to(self.project_path)).replace(os.sep, '/')
                    _, matched_pattern = self._classify_path(file_path)

                    try:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as in_f:
                            content = in_f.read()
                        
                        out_f.write(f"--- START OF FILE {relative_path_str} ---\n")
                        out_f.write(f"# Category: {category}\n")
                        out_f.write(f"# Matched by pattern: {matched_pattern}\n")
                        out_f.write(f"# ---\n")
                        out_f.write(content.strip())
                        out_f.write(f"\n--- END OF FILE {relative_path_str} ---\n\n")
                        self._log(f"[+] Captured: {relative_path_str}", indent=3)

                    except Exception as e:
                        self._log(f"[!] Failed to read {relative_path_str}: {e}", indent=3)

    def _print_summary(self, duration):
        total_captured = sum(len(files) for files in self.files_by_category.values())
        print("\n" + "="*80)
        self._log("âœ… [Architect's Insight] æ‰«æå®Œæˆ!")
        self._log(f"â±ï¸  è€—æ—¶: {duration:.2f} ç§’")
        self._log(f"ğŸ“ æ‰«ææ–‡ä»¶æ€»æ•°: {self.total_files_scanned}")
        self._log(f"ğŸ¯ æ•è·æ ¸å¿ƒæ–‡ä»¶æ•°: {total_captured}")
        self._log(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {self.output_file}")
        print("-"*80)
        
        self._log("æ•è·æ–‡ä»¶åˆ†ç±»ç»Ÿè®¡:")
        category_order = self.config.get('category_order', [])
        for category in category_order:
            if category in self.files_by_category:
                count = len(self.files_by_category[category])
                self._log(f"  - {category}: {count} ä¸ªæ–‡ä»¶", indent=1)
        print("="*80)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="V7.1 - Robust Traversal: å¯¹é¡¹ç›®è¿›è¡Œæ·±åº¦æ¶æ„æ‰«æï¼Œæå–å…³é”®æ–‡ä»¶å¹¶é™„åŠ ä¸Šä¸‹æ–‡ã€‚"
    )
    parser.add_argument(
        "project_path", nargs='?', default='.', 
        help="è¦æ‰«æçš„é¡¹ç›®æ ¹ç›®å½•è·¯å¾„ (é»˜è®¤ä¸ºå½“å‰ç›®å½•)ã€‚"
    )
    parser.add_argument(
        "--output", default="monorepo_snapshot_architect.txt", 
        help="è¾“å‡ºæ–‡ä»¶çš„åç§°ã€‚"
    )
    parser.add_argument(
        "--config", default="scraper_config.yaml",
        help="é…ç½®æ–‡ä»¶çš„è·¯å¾„ã€‚"
    )
    
    args = parser.parse_args()
    
    scraper = ProjectScraper(args.project_path, args.output, args.config)
    scraper.scrape()